# ðŸ¤–âš¡ My Natural Language Processing Notebooks

Connect with me on [LinkedIn](https://www.linkedin.com/in/nakshatrasinghh/).

ðŸ‘‰ If you like my work, check out my other [Repositories!](https://github.com/nakshatrasinghh?tab=repositories) ðŸ‘ˆ

Click <img src="https://colab.research.google.com/assets/colab-badge.svg" align="top"> to view the **Jupyter notebook** in Google Colab:

\# | Description | Link
--- | --- | ---
1 | [How to create a Custom Dataset using Google Play Apps Scrapper script for Sentiment Analysis](https://github.com/nakshatrasinghh/Natural-Language-Processing/blob/master/Custom_Dataset_Sentiment.ipynb) | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/nakshatrasinghh/Natural-Language-Processing/blob/master/Custom_Dataset_Sentiment.ipynb)
2 | [Do Pretrained Embeddings really give you an edge in Training NLP Models? (GloVe and Fasttext)](https://github.com/nakshatrasinghh/Natural-Language-Processing/blob/master/Do_Embeddings_Give_You_An_Edge.ipynb) | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/nakshatrasinghh/Natural-Language-Processing/blob/master/Do_Embeddings_Give_You_An_Edge.ipynb)
3 | [Fake News Detection using Simple Recurrent Neural Network (RNN) and NLTK (Tensorflow)](https://github.com/nakshatrasinghh/Natural-Language-Processing/blob/master/Fake_News_Detection%2BRNN.ipynb) | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/nakshatrasinghh/Natural-Language-Processing/blob/master/Fake_News_Detection%2BRNN.ipynb)
4 | [Introduction to BERT with Ktrain using IMDB Movie Review Sentiment dataset](https://github.com/nakshatrasinghh/Natural-Language-Processing/blob/master/KTrain%2BBERT.ipynb) | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/nakshatrasinghh/Natural-Language-Processing/blob/master/KTrain%2BBERT.ipynb)
5 | [English to Danish Machine Translation using Gated Recurrent Unit (GRU)](https://github.com/nakshatrasinghh/Natural-Language-Processing/blob/master/Machine_Translation_using_GRUs.ipynb) | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/nakshatrasinghh/Natural-Language-Processing/blob/master/Machine_Translation_using_GRUs.ipynb)
6 | [Question Answering with a Fine-Tuned BERT using Stanford Question Answering Dataset (SQuAD)](https://github.com/nakshatrasinghh/Natural-Language-Processing/blob/master/Question_Answering_with_BERT.ipynb) | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/nakshatrasinghh/Natural-Language-Processing/blob/master/Question_Answering_with_BERT.ipynb)
7 | [Building a BERT model for Sentiment Analysis using Custom Google Play Apps Reviews Dataset (Pytorch)](https://github.com/nakshatrasinghh/Natural-Language-Processing/blob/master/Sentiment_Analysis_with_BERT.ipynb) | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/nakshatrasinghh/Natural-Language-Processing/blob/master/Sentiment_Analysis_with_BERT.ipynb)
8 | [Building a BERT model for Text Classification using IMDB Reviews Dataset (Tensorflow)](https://github.com/nakshatrasinghh/Natural-Language-Processing/blob/master/Text_Classification_BERT%2BTF.ipynb) | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/nakshatrasinghh/Natural-Language-Processing/blob/master/Text_Classification_BERT%2BTF.ipynb)
9 | [Classifying Toxic Comments with BERT using Jigsaw Toxic Comment Classification Dataset (Pytorch)](https://github.com/nakshatrasinghh/Natural-Language-Processing/blob/master/Toxic_Comment_Classification%2BBERT.ipynb) | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/nakshatrasinghh/Natural-Language-Processing/blob/master/Toxic_Comment_Classification%2BBERT.ipynb)
10 | [Disaster Tweets Classification with SVM and BERT with Exploratory Data Analysis (Pytorch)](https://github.com/nakshatrasinghh/Natural-Language-Processing/blob/master/Disaster_Tweets_Classification_with_SVM_and_BERT.ipynb) | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/nakshatrasinghh/Natural-Language-Processing/blob/master/Disaster_Tweets_Classification_with_SVM_and_BERT.ipynb)


# ðŸ‘¾PyTorch
PyTorch-Transformers (formerly known as pytorch-pretrained-bert) is a library of state-of-the-art pre-trained models for Natural Language Processing (NLP)
The library currently contains PyTorch implementations, pre-trained model weights, usage scripts and conversion utilities for the following models:
- [BERT](https://github.com/google-research/bert) (from Google) released with the paper [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805) by Jacob Devlin, Ming-Wei Chang, Kenton Lee and Kristina Toutanova
- [GPT](https://github.com/openai/finetune-transformer-lm) (from OpenAI) released with the paper [Improving Language Understanding by Generative Pre-Training](https://blog.openai.com/language-unsupervised/) by Alec Radford, Karthik Narasimhan, Tim Salimans and Ilya Sutskever
- [GPT-2](https://blog.openai.com/better-language-models/) (from OpenAI) released with the paper [Language Models are Unsupervised Multitask Learners[(https://blog.openai.com/better-language-models/) by Alec Radford*, Jeffrey Wu*, Rewon Child, David Luan, Dario Amodei** and Ilya Sutskever**.
- [Transformer-XL](https://github.com/kimiyoung/transformer-xl) (from Google/CMU) released with the paper [Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context](https://arxiv.org/abs/1901.02860) by Zihang Dai*, Zhilin Yang*, Yiming Yang, Jaime Carbonell, Quoc V. Le, Ruslan Salakhutdinov
- [XLNet](https://github.com/zihangdai/xlnet/) (from Google/CMU) released with the paper [XLNet: Generalized Autoregressive Pretraining for Language Understanding](https://arxiv.org/abs/1906.08237) by Zhilin Yang*, Zihang Dai*, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov, Quoc V. Le
- [XLM](https://github.com/facebookresearch/XLM/) (from Facebook) released together with the paper [Cross-lingual Language Model Pretraining](https://arxiv.org/abs/1901.07291) by Guillaume Lample and Alexis Conneau
![]()

# Message Me
Click on the icon to personally message me on Whatsapp for future collaborations or projects :)
</a>
<a href="https://wa.link/8bt67v">
  <img align="left" alt="Nakshatra's Whatsapp" width="30px" src="https://image.flaticon.com/icons/svg/785/785767.svg" />
</a>
