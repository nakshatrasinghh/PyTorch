{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Duplicate-Question-Recognition.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7rj5f0PfZRI5"
      },
      "source": [
        "# ***Duplicate Questions Recognition using LSTM and Trax***\n",
        "By Nakshatra Singh\n",
        "\n",
        "This notebook is an illustration on how to build a LSTM Model using Trax which can identify the Duplicate Questions or Similar Questions which is useful when we have to work with several versions of the same Questions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rwdiWZT19PF3"
      },
      "source": [
        "###**Using Google GPU for Training**\n",
        "\n",
        "Google colab offers free GPUs and TPUs! Since we'll be training a large model it's best to take advantage of this (in this case we'll use GPU), otherwise training can take long time.\n",
        "\n",
        "A GPU can be added by going to the menu and selecting:\n",
        "\n",
        "`Edit -> Notebook Settings -> Hardware Accelerator -> (GPU)`\n",
        "\n",
        "Then run the following cell to confirm that a GPU is detected. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ae6JgckZ9QBv",
        "outputId": "33dfc2c0-f633-4fbd-a3d1-51de61bf56cf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "# Get the device GPU name \n",
        "device_name = tf.test.gpu_device_name()\n",
        "\n",
        "# The device name should look like the following:\n",
        "if device_name == '/device:GPU:0':\n",
        "  print('Found GPU at : {}'.format(device_name)) \n",
        "else:\n",
        "  raise SystemError('GPU not found!')  "
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found GPU at : /device:GPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vJk-lKneaHs4"
      },
      "source": [
        "###**Setting up Imports and Downloading the Dependencies**\n",
        "\n",
        "I have downloaded all the Libraries and Dependencies required for this Project in one particular cell. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VrTT3iWQZNSa",
        "outputId": "3dc81267-0827-4ef7-f2ac-628e21928fa9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 242
        }
      },
      "source": [
        "#@ Downloading the Libraries and Dependencies. \n",
        "\n",
        "!pip install -q -U trax                   # Downloading the Trax.\n",
        "import nltk                       \n",
        "nltk.download(\"punkt\")\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import trax\n",
        "from trax import layers as tl\n",
        "from trax.supervised import training\n",
        "from trax.fastmath import numpy as fastnp\n",
        "import random\n",
        "from collections import defaultdict\n",
        "from functools import partial "
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 471kB 4.9MB/s \n",
            "\u001b[K     |████████████████████████████████| 2.6MB 19.4MB/s \n",
            "\u001b[K     |████████████████████████████████| 174kB 41.7MB/s \n",
            "\u001b[K     |████████████████████████████████| 71kB 10.4MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.1MB 43.9MB/s \n",
            "\u001b[K     |████████████████████████████████| 348kB 53.9MB/s \n",
            "\u001b[K     |████████████████████████████████| 3.6MB 49.1MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.3MB 58.1MB/s \n",
            "\u001b[K     |████████████████████████████████| 2.9MB 50.4MB/s \n",
            "\u001b[K     |████████████████████████████████| 890kB 54.8MB/s \n",
            "\u001b[?25h  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HCo-b0HGblhx"
      },
      "source": [
        "###**Retrieve Dataset**\n",
        "\n",
        "Let's download the dataset which is uploaded on my google drive. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TNcLhjiyaZEl",
        "outputId": "cb846b8f-e8c9-40e0-b8d5-009c9be19832",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        }
      },
      "source": [
        "!gdown --id 1USFKeVRsuai_62I_tYa_RbSPOhBtJx8G "
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1USFKeVRsuai_62I_tYa_RbSPOhBtJx8G\n",
            "To: /content/Questions.csv\n",
            "60.7MB [00:00, 107MB/s] \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TkDwnqzubith",
        "outputId": "1e15af75-0637-491d-bf7b-bfc4f2564ce2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 236
        }
      },
      "source": [
        "data = pd.read_csv('/content/Questions.csv')\n",
        "print(f\"Number of Question Pairs: {len(data)}\")\n",
        "print() \n",
        "data.head() "
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of Question Pairs: 404351\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>qid1</th>\n",
              "      <th>qid2</th>\n",
              "      <th>question1</th>\n",
              "      <th>question2</th>\n",
              "      <th>is_duplicate</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>What is the step by step guide to invest in sh...</td>\n",
              "      <td>What is the step by step guide to invest in sh...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>What is the story of Kohinoor (Koh-i-Noor) Dia...</td>\n",
              "      <td>What would happen if the Indian government sto...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>How can I increase the speed of my internet co...</td>\n",
              "      <td>How can Internet speed be increased by hacking...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>7</td>\n",
              "      <td>8</td>\n",
              "      <td>Why am I mentally very lonely? How can I solve...</td>\n",
              "      <td>Find the remainder when [math]23^{24}[/math] i...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>9</td>\n",
              "      <td>10</td>\n",
              "      <td>Which one dissolve in water quikly sugar, salt...</td>\n",
              "      <td>Which fish would survive in salt water?</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   id  qid1  ...                                          question2 is_duplicate\n",
              "0   0     1  ...  What is the step by step guide to invest in sh...            0\n",
              "1   1     3  ...  What would happen if the Indian government sto...            0\n",
              "2   2     5  ...  How can Internet speed be increased by hacking...            0\n",
              "3   3     7  ...  Find the remainder when [math]23^{24}[/math] i...            0\n",
              "4   4     9  ...            Which fish would survive in salt water?            0\n",
              "\n",
              "[5 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RLuv43E3FV_0"
      },
      "source": [
        "I will split the Data into Training set and Testing Set. The Test Set will be used later to evaluate the Model. I will select only the Question Pairs that are duplicate to train the Model. I will build two batches as input for the Neural Networks: Siamese Networks. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NhieNxfAb9a_",
        "outputId": "66f73ca9-84b2-49d2-fe56-de97d100d22e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "#@ Processing the Data:\n",
        "N_train = 300000                                               \n",
        "N_test = 10240                                                 \n",
        "data_train = data[:N_train]                                                    # Training pairs.\n",
        "data_test = data[N_train:N_train+N_test]                                       # Test pairs.\n",
        "del(data)                                                                      # Removing.\n",
        "\n",
        "#@ Inspecting the Data:\n",
        "print(f\"Training Set: {len(data_train)} and Test Set: {len(data_test)}\")\n",
        "\n",
        "#@ Selecting the Question Pairs for Training:\n",
        "train_idx = (data_train[\"is_duplicate\"] == 1).to_numpy()\n",
        "train_idx = [i for i, x in enumerate(train_idx) if x]\n",
        "print(f\"Number of Duplicate Questions: {len(train_idx)}\")"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training Set: 300000 and Test Set: 10240\n",
            "Number of Duplicate Questions: 111486\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pdZRH5yQFhuG"
      },
      "source": [
        "###**Preparing the Data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Un33Od3rEAMX",
        "outputId": "f26be24f-b359-4e6b-d41d-7701c7904e63",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190
        }
      },
      "source": [
        "#@ Preparing the Data: Training the Model: \n",
        "Q1_train_words = np.array(data_train['question1'][train_idx])\n",
        "Q2_train_words = np.array(data_train['question2'][train_idx])\n",
        "\n",
        "#@ Preparing the Data: Evaluating the Model:\n",
        "Q1_test_words = np.array(data_test['question1'])\n",
        "Q2_test_words = np.array(data_test['question2'])\n",
        "y_test = np.array(data_test[\"is_duplicate\"]) \n",
        "\n",
        "#@ Inspecting the Data:\n",
        "print(\"TRAINING QUESTIONS:\\n\")\n",
        "print(\"Question 1:\", Q1_train_words[1])\n",
        "print(\"Question 2:\", Q2_train_words[1], \"\\n\")\n",
        "\n",
        "print(\"TESTING QUESTIONS:\\n\")\n",
        "print(\"Question 1:\", Q1_test_words[1])\n",
        "print(\"Question 2:\", Q2_test_words[1], \"\\n\") "
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TRAINING QUESTIONS:\n",
            "\n",
            "Question 1: How can I be a good geologist?\n",
            "Question 2: What should I do to be a great geologist? \n",
            "\n",
            "TESTING QUESTIONS:\n",
            "\n",
            "Question 1: What is the best bicycle to buy under 10k?\n",
            "Question 2: Which is the best bike in in dia to buy in INR 10k? \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SYm4-01jFrA1"
      },
      "source": [
        "I will encode each word of the selected pairs with an Index which will be a list of numbers. Firstly, I will Tokenize each word using NLTK and I will use Python's Default Dictionary which assigns the values 0 to all Out of Vocabulary Words. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rMPeMlTHDo0w",
        "outputId": "8fc9598b-619b-48f2-cf53-effd194824bf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "'''Sit back and enjoy your Coffee'''\n",
        "#@ Preparing the Data:\n",
        "Q1_train = np.empty_like(Q1_train_words)                             # Creating new Training array.\n",
        "Q2_train = np.empty_like(Q2_train_words)                             # Creating new Training array.\n",
        "Q1_test = np.empty_like(Q1_test_words)                               # Creating new Test array.\n",
        "Q2_test = np.empty_like(Q2_test_words)                               # Creating new Test array.\n",
        "\n",
        "#@ Building Vocabulary with Training Dataset:\n",
        "vocab = defaultdict(lambda: 0)                                       # It will create a dict with default 0 when a key doesn't exist.\n",
        "vocab[\"<PAD>\"] = 1\n",
        "for idx in range(len(Q1_train_words)):\n",
        "  Q1_train[idx] = nltk.word_tokenize(Q1_train_words[idx])            # Tokenizing the Training Set.\n",
        "  Q2_train[idx] = nltk.word_tokenize(Q2_train_words[idx])            # Tokenizing the Training Set.\n",
        "  q = Q1_train[idx] + Q2_train[idx]                                  # Adding Q1 and Q2 pair of tokens togethers.\n",
        "  for word in q:\n",
        "    if word not in vocab:\n",
        "      vocab[word] = len(vocab) + 1\n",
        "print(\"The length of the Vocabulary is:\", len(vocab))\n",
        "\n",
        "#@ Testing Dataset:\n",
        "for idx in range(len(Q1_test_words)):\n",
        "  Q1_test[idx] = nltk.word_tokenize(Q1_test_words[idx])              # Tokenizing the Test Set.\n",
        "  Q2_test[idx] = nltk.word_tokenize(Q2_test_words[idx])              # Tokenizing the Test Set.\n",
        "\n",
        "#@ Inspecting the Final Prepared Dataset:\n",
        "print(\"Training Set is reduced to:\", len(Q1_train))\n",
        "print(\"Test Set is:\", len(Q1_test))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The length of the Vocabulary is: 36342\n",
            "Training Set is reduced to: 111486\n",
            "Test Set is: 10240\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iGh_Qfe0GNF_",
        "outputId": "4384dacc-4c41-45c8-8f39-fe1d92b2c5e1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 311
        }
      },
      "source": [
        "#@ Preparing the Data:\n",
        "\n",
        "#@ Converting Questions pairs to array of Integers:\n",
        "for i in range(len(Q1_train)):\n",
        "  Q1_train[i] = [vocab[word] for word in Q1_train[i]]\n",
        "  Q2_train[i] = [vocab[word] for word in Q2_train[i]]\n",
        "\n",
        "#@ Converting Questions pairs to array of Integers:\n",
        "for i in range(len(Q1_test)):\n",
        "  Q1_test[i] = [vocab[word] for word in Q1_test[i]]\n",
        "  Q2_test[i] = [vocab[word] for word in Q2_test[i]]\n",
        "\n",
        "#@ Inspecting the Encoded Data:\n",
        "print(\"Question in the Training Set:\")                           # Inspecting the Training Set.\n",
        "print(Q1_train_words[1], \"\\n\")\n",
        "print(\"Encoded Version:\")\n",
        "print(Q1_train[1], \"\\n\")\n",
        "print(\"Question in the Test Set:\")                               # Inspecting the Test Set.\n",
        "print(Q1_test_words[1], \"\\n\")\n",
        "print(\"Encoded Version:\")\n",
        "print(Q1_test[1], \"\\n\")\n",
        "\n",
        "#@ Splitting the Training Set into Training and Validation Dataset:\n",
        "split = int(len(Q1_train) * 0.8)\n",
        "train_Q1, train_Q2 = Q1_train[:split], Q2_train[:split]                        # Split for Training set.\n",
        "val_Q1, val_Q2 = Q1_train[split:], Q2_train[split:]                            # Split for Validation set.\n",
        "print(f\"Total numbers of questions pairs: {len(Q1_train)}\")  \n",
        "print()            \n",
        "print(f\"The length of Training set: {len(train_Q1)}\")                          # Length of Final Training set.\n",
        "print()                          \n",
        "print(f\"The length of Validation set: {len(val_Q1)}\")                          # Length of Final Validation set. "
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Question in the Training Set:\n",
            "How can I be a good geologist? \n",
            "\n",
            "Encoded Version:\n",
            "[32, 33, 4, 34, 6, 35, 36, 21] \n",
            "\n",
            "Question in the Test Set:\n",
            "What is the best bicycle to buy under 10k? \n",
            "\n",
            "Encoded Version:\n",
            "[30, 156, 78, 216, 8914, 39, 716, 286, 8324, 21] \n",
            "\n",
            "Total numbers of questions pairs: 111486\n",
            "\n",
            "The length of Training set: 89188\n",
            "\n",
            "The length of Validation set: 22298\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PoDpqEPxQTrb"
      },
      "source": [
        "###**Data Generator**\n",
        "\n",
        "In most of the NLP, ML and DL in general, using batches when training the Dataset is more efficient. Now, I will build the Data Generator that takes in Questions pairs and returns batches in the form of Tuples. The Tuples consist of two arrays and each array will have batch size Questions pairs. The command next (data generator) will return the next batch. The Data Generator will returns the Data in a format that can be used directly int the Model while computing Feed Forward. It will return a pair of arrays of Questions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VoxO9MWTIkKW",
        "outputId": "4d0cdd27-0c41-4e9c-f8d9-311c1aadf992",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 277
        }
      },
      "source": [
        "#@ Data Generator:\n",
        "def data_generator(Q1, Q2, batch_size, pad=1, shuffle=True):\n",
        "  \"\"\" Generator Function that yields the Batches of Data. \"\"\"\n",
        "  #@ Initializing the Dependencies:\n",
        "  input1, input2 = [], []\n",
        "  idx = 0\n",
        "  len_q = len(Q1)\n",
        "  question_index = [*range(len_q)]\n",
        "  if shuffle:\n",
        "    random.shuffle(question_index)\n",
        "  \n",
        "  while True:\n",
        "    if idx >= len_q:\n",
        "      idx = 0\n",
        "      if shuffle:\n",
        "        random.shuffle(question_index)\n",
        "    #@ Getting the Questions pairs in Index positions:\n",
        "    q1 = Q1[question_index[idx]]\n",
        "    q2 = Q2[question_index[idx]]\n",
        "    idx += 1\n",
        "    #@ Adding the Data:\n",
        "    input1.append(q1)\n",
        "    input2.append(q2)\n",
        "    if len(input1) == batch_size:\n",
        "      max_len = max(max([len(q) for q in input1]),\n",
        "                    max([len(q) for q in input2]))\n",
        "      max_len = 2**int(np.ceil(np.log2(max_len)))\n",
        "      b1, b2 = [], []\n",
        "      for q1, q2 in zip(input1, input2):\n",
        "        q1 = q1 + [pad] * (max_len - len(q1))                         # Adding pad to q1 until it reaches max length.\n",
        "        q2 = q2 + [pad] * (max_len - len(q2))                         # Adding pad to q2 until it reaches max length.\n",
        "        b1.append(q1)\n",
        "        b2.append(q2)\n",
        "      yield np.array(b1), np.array(b2)\n",
        "      input1, input2 = [], []                                         # Resetting the Batches.\n",
        "\n",
        "#@ Inspecting the Example of Data Generator:\n",
        "res1, res2 = next(data_generator(train_Q1, train_Q2, batch_size=2))\n",
        "print(f\"First Questions:\\n{res1}\")\n",
        "print(f\"\\nSecond Questions:\\n{res2}\") "
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "First Questions:\n",
            "[[   30    16    73  7516 10298    38    21     1     1     1     1     1\n",
            "      1     1     1     1     1     1     1     1     1     1     1     1\n",
            "      1     1     1     1     1     1     1     1]\n",
            " [   30    16     6 11900  1085    38    21    86     6 11900  1085  4838\n",
            "     39   473    31   260    21     1     1     1     1     1     1     1\n",
            "      1     1     1     1     1     1     1     1]]\n",
            "\n",
            "Second Questions:\n",
            "[[   32    38  7513 15792   302    21     1     1     1     1     1     1\n",
            "      1     1     1     1     1     1     1     1     1     1     1     1\n",
            "      1     1     1     1     1     1     1     1]\n",
            " [   30   156 11900   421    11    15  3089   131   302    38   276  1108\n",
            "     38    21     1     1     1     1     1     1     1     1     1     1\n",
            "      1     1     1     1     1     1     1     1]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EmsBCZR181Kr"
      },
      "source": [
        "###**Siamese Neural Network**\n",
        "\n",
        "A Siamese Neural Network is a Neural Network which uses the same weight while working in tandem on two different Input vectors to compute comparable output Vectors. Here, I will get the Embedding, run it through LSTM or Long Short Term Memory Network, Noramlize the two Vectors and Finally, I will use Triplet Loss to get the corresponding Cosine Similarity for each pair of Questions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jRaS_UXq8Qoy",
        "outputId": "7d5df8e9-17f4-4a41-dc32-b0821190faa6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 259
        }
      },
      "source": [
        "#@ Siamese Neural Network using Trax:\n",
        "def Siamese(vocab_size=len(vocab), d_model=128, mode=\"train\"):\n",
        "  \"\"\" Returns a Siamese Model. \"\"\"\n",
        "  #@ Normalizing the Vectors for L2 Normalization:\n",
        "  def normalize(x):\n",
        "    return x / fastnp.sqrt(fastnp.sum(x*x, axis=-1, keepdims=True))\n",
        "  #@ Preparing the Model:\n",
        "  processor = tl.Serial(                                                  # Returns one hot Vector.\n",
        "      tl.Embedding(vocab_size=vocab_size, d_feature=d_model),             # Adding Embedding Layer.\n",
        "      tl.LSTM(n_units=d_model),                                           # Adding the LSTM Layer.\n",
        "      tl.Mean(axis=1),                                                    # Mean over Columns in Neural Networks.\n",
        "      # tl.Dense(n_units=vocab_size),                                     # Adding a Dense Layer.\n",
        "      tl.Fn(\"Normalize\", lambda x: normalize(x))                          # Adding the Normalizing Function.\n",
        "  )\n",
        "  #@ Running the Model in parallel:\n",
        "  model = tl.Parallel(processor, processor)\n",
        "  return model\n",
        "\n",
        "#@ Setting up Siamese Neural Network Model:\n",
        "model = Siamese()\n",
        "print(model)                                                              # Inspecting the Model."
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Parallel_in2_out2[\n",
            "  Serial[\n",
            "    Embedding_41789_128\n",
            "    LSTM_128\n",
            "    Mean\n",
            "    Normalize\n",
            "  ]\n",
            "  Serial[\n",
            "    Embedding_41789_128\n",
            "    LSTM_128\n",
            "    Mean\n",
            "    Normalize\n",
            "  ]\n",
            "]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "muk3BvIY8-4M"
      },
      "source": [
        "###**Triplet Loss**\n",
        "\n",
        "The Triplet Loss makes use of a Baseline or Anchor Input which is compared to the Positive or Truthy Input and a Negatve or Falsy Input. The distance from the Anchor Input to the Positive Input is minimized and the distance from the Anchor Input to the Negative Input is maximized. The Triplet Loss is composed of two terms where one term utilizes the mean of all the non duplicates and the second term utilizes the Closest Negative. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p8Ocb_QB88sA"
      },
      "source": [
        "#@ Triplet Loss Function:\n",
        "def TripletLossFn(v1, v2, margin=0.25):\n",
        "  \"\"\" Custom Loss Function. \"\"\"\n",
        "  scores = fastnp.dot(v1, v2.T)                                                       # Calculating the dot product of two batches.\n",
        "  batch_size = len(scores)                                                            # Calculating the new batch size.\n",
        "  positive = fastnp.diagonal(scores)                                                  # Getting positive diagonal entries in scores.\n",
        "  negative_without_positive = scores - 2.0 * fastnp.eye(batch_size)\n",
        "  closest_negative = negative_without_positive.max(axis=1)                            # Taking row by row max.\n",
        "  negative_zero_on_duplicate = scores * (1.0 - fastnp.eye(batch_size))\n",
        "  mean_negative = fastnp.sum(negative_zero_on_duplicate, axis=1)/(batch_size - 1)\n",
        "  triplet_loss1 = fastnp.maximum(0, margin - positive + closest_negative)\n",
        "  triplet_loss2 = fastnp.maximum(0, margin - positive + mean_negative)\n",
        "  triplet_loss = fastnp.mean(triplet_loss1 + triplet_loss2)\n",
        "  return triplet_loss\n",
        "\n",
        "#@ Triplet Loss:\n",
        "def TripletLoss(margin=0.25):\n",
        "  triplet_loss_fn = partial(TripletLossFn, margin=margin)\n",
        "  return tl.Fn(\"TripletLoss\", triplet_loss_fn) "
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mgL0XCEz9Ezy"
      },
      "source": [
        "###**Model Training**\n",
        "\n",
        "Now, I will train the Model. I will define the Cost Function and the Optimizer as ususal. I will use Training Iterator to go through all the Data for each Epochs while training the Model. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8pFgWFy69DHG",
        "outputId": "c25a215c-36c5-41da-a61c-ad83f2a9430f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#@ Preparing the Data:\n",
        "batch_size = 256\n",
        "train_generator = data_generator(train_Q1, train_Q2, batch_size, vocab[\"<PAD>\"])\n",
        "val_generator = data_generator(val_Q1, val_Q2, batch_size, vocab[\"<PAD>\"])\n",
        "\n",
        "#@ Training the Model:\n",
        "lr_schedule = trax.lr.warmup_and_rsqrt_decay(400, 0.01)\n",
        "\n",
        "def train_model(Siamese, TripletLoss, lr_schedule, train_generator=train_generator,\n",
        "                val_generator=val_generator, output_dir=\"model/\"):\n",
        "  \"\"\" Training the Siamese Model. \"\"\"\n",
        "  output_dir = os.path.expanduser(output_dir)\n",
        "  \n",
        "  #@ Training:\n",
        "  train_task = training.TrainTask(\n",
        "      labeled_data = train_generator,                                                   # Using Train Generator.\n",
        "      loss_layer = TripletLoss(),                                                       # Using Triplet Loss Function.\n",
        "      optimizer = trax.optimizers.Adam(0.001),                                          # Using Adam Optimizer.\n",
        "      lr_schedule = lr_schedule                                                         # Using Trax Multifactor Schedule Function.\n",
        "  )\n",
        "  #@ Evaluating:\n",
        "  eval_task = training.EvalTask(\n",
        "      labeled_data = val_generator,                                                     # Using Validation Generator.\n",
        "      metrics = [TripletLoss()],                                                        # Instantiating the Objects for Evaluation.\n",
        "      n_eval_batches = 3\n",
        "  )\n",
        "  #@ Training the Model:\n",
        "  training_loop = training.Loop(                                                        # Training the Model.\n",
        "      Siamese(),                                                                        # Siameses Neural Networks.\n",
        "      train_task, eval_tasks = eval_task,\n",
        "      output_dir = output_dir\n",
        "  )\n",
        "  return training_loop\n",
        "\n",
        "#@ Training the Model:\n",
        "training_loop = train_model(Siamese, TripletLoss, lr_schedule)\n",
        "training_loop.run(3000)                                                                 # Training for 1000 epochs."
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Step      1: Total number of trainable weights: 5480576\n",
            "Step      1: Ran 1 train steps in 5.37 secs\n",
            "Step      1: train TripletLoss |  0.49999863\n",
            "Step      1: eval  TripletLoss |  0.49999818\n",
            "\n",
            "Step    100: Ran 99 train steps in 8.27 secs\n",
            "Step    100: train TripletLoss |  0.49995661\n",
            "Step    100: eval  TripletLoss |  0.49858472\n",
            "\n",
            "Step    200: Ran 100 train steps in 5.26 secs\n",
            "Step    200: train TripletLoss |  0.49985439\n",
            "Step    200: eval  TripletLoss |  0.49999246\n",
            "\n",
            "Step    300: Ran 100 train steps in 7.15 secs\n",
            "Step    300: train TripletLoss |  0.49998352\n",
            "Step    300: eval  TripletLoss |  0.49999899\n",
            "\n",
            "Step    400: Ran 100 train steps in 5.45 secs\n",
            "Step    400: train TripletLoss |  0.49998981\n",
            "Step    400: eval  TripletLoss |  0.49996084\n",
            "\n",
            "Step    500: Ran 100 train steps in 5.55 secs\n",
            "Step    500: train TripletLoss |  0.49998331\n",
            "Step    500: eval  TripletLoss |  0.49999070\n",
            "\n",
            "Step    600: Ran 100 train steps in 5.50 secs\n",
            "Step    600: train TripletLoss |  0.49837515\n",
            "Step    600: eval  TripletLoss |  0.49811158\n",
            "\n",
            "Step    700: Ran 100 train steps in 5.55 secs\n",
            "Step    700: train TripletLoss |  0.42934027\n",
            "Step    700: eval  TripletLoss |  0.37706804\n",
            "\n",
            "Step    800: Ran 100 train steps in 5.47 secs\n",
            "Step    800: train TripletLoss |  0.35025528\n",
            "Step    800: eval  TripletLoss |  0.33703130\n",
            "\n",
            "Step    900: Ran 100 train steps in 5.52 secs\n",
            "Step    900: train TripletLoss |  0.31015435\n",
            "Step    900: eval  TripletLoss |  0.29333973\n",
            "\n",
            "Step   1000: Ran 100 train steps in 5.46 secs\n",
            "Step   1000: train TripletLoss |  0.28589049\n",
            "Step   1000: eval  TripletLoss |  0.27775735\n",
            "\n",
            "Step   1100: Ran 100 train steps in 5.50 secs\n",
            "Step   1100: train TripletLoss |  0.27246368\n",
            "Step   1100: eval  TripletLoss |  0.26533145\n",
            "\n",
            "Step   1200: Ran 100 train steps in 5.54 secs\n",
            "Step   1200: train TripletLoss |  0.26141879\n",
            "Step   1200: eval  TripletLoss |  0.25783411\n",
            "\n",
            "Step   1300: Ran 100 train steps in 5.51 secs\n",
            "Step   1300: train TripletLoss |  0.24650523\n",
            "Step   1300: eval  TripletLoss |  0.23109722\n",
            "\n",
            "Step   1400: Ran 100 train steps in 5.56 secs\n",
            "Step   1400: train TripletLoss |  0.19595523\n",
            "Step   1400: eval  TripletLoss |  0.16534865\n",
            "\n",
            "Step   1500: Ran 100 train steps in 5.49 secs\n",
            "Step   1500: train TripletLoss |  0.13894755\n",
            "Step   1500: eval  TripletLoss |  0.12326863\n",
            "\n",
            "Step   1600: Ran 100 train steps in 5.48 secs\n",
            "Step   1600: train TripletLoss |  0.11546372\n",
            "Step   1600: eval  TripletLoss |  0.11456445\n",
            "\n",
            "Step   1700: Ran 100 train steps in 5.49 secs\n",
            "Step   1700: train TripletLoss |  0.10180593\n",
            "Step   1700: eval  TripletLoss |  0.09633033\n",
            "\n",
            "Step   1800: Ran 100 train steps in 5.39 secs\n",
            "Step   1800: train TripletLoss |  0.07933094\n",
            "Step   1800: eval  TripletLoss |  0.09296034\n",
            "\n",
            "Step   1900: Ran 100 train steps in 5.48 secs\n",
            "Step   1900: train TripletLoss |  0.07022714\n",
            "Step   1900: eval  TripletLoss |  0.08511063\n",
            "\n",
            "Step   2000: Ran 100 train steps in 5.51 secs\n",
            "Step   2000: train TripletLoss |  0.06567862\n",
            "Step   2000: eval  TripletLoss |  0.08809952\n",
            "\n",
            "Step   2100: Ran 100 train steps in 5.55 secs\n",
            "Step   2100: train TripletLoss |  0.06400333\n",
            "Step   2100: eval  TripletLoss |  0.07271387\n",
            "\n",
            "Step   2200: Ran 100 train steps in 5.45 secs\n",
            "Step   2200: train TripletLoss |  0.05168796\n",
            "Step   2200: eval  TripletLoss |  0.07899173\n",
            "\n",
            "Step   2300: Ran 100 train steps in 5.50 secs\n",
            "Step   2300: train TripletLoss |  0.05093256\n",
            "Step   2300: eval  TripletLoss |  0.07852908\n",
            "\n",
            "Step   2400: Ran 100 train steps in 5.32 secs\n",
            "Step   2400: train TripletLoss |  0.05160369\n",
            "Step   2400: eval  TripletLoss |  0.06864272\n",
            "\n",
            "Step   2500: Ran 100 train steps in 5.53 secs\n",
            "Step   2500: train TripletLoss |  0.04538853\n",
            "Step   2500: eval  TripletLoss |  0.07128575\n",
            "\n",
            "Step   2600: Ran 100 train steps in 5.44 secs\n",
            "Step   2600: train TripletLoss |  0.04381843\n",
            "Step   2600: eval  TripletLoss |  0.07731831\n",
            "\n",
            "Step   2700: Ran 100 train steps in 5.34 secs\n",
            "Step   2700: train TripletLoss |  0.04567402\n",
            "Step   2700: eval  TripletLoss |  0.07448425\n",
            "\n",
            "Step   2800: Ran 100 train steps in 5.49 secs\n",
            "Step   2800: train TripletLoss |  0.04304573\n",
            "Step   2800: eval  TripletLoss |  0.06583527\n",
            "\n",
            "Step   2900: Ran 100 train steps in 5.40 secs\n",
            "Step   2900: train TripletLoss |  0.03916567\n",
            "Step   2900: eval  TripletLoss |  0.06667005\n",
            "\n",
            "Step   3000: Ran 100 train steps in 5.51 secs\n",
            "Step   3000: train TripletLoss |  0.03948541\n",
            "Step   3000: eval  TripletLoss |  0.06866615\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "485jgYNe9uCJ"
      },
      "source": [
        "###**Model Evaluation**\n",
        "\n",
        "I will utilize the Test Set which was configured earlier to determine the accuracy of the Model. Actually the Training Set only had Positive examples whereas the Test Set and y test is setup as pairs of Questions and some of which are duplicates and some are not. I will compute the Cosine Similarity of each pair, threshold it and compare the result to y test. The results are accumulated to produce the Accuracy. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KSRMxWWM9ItW",
        "outputId": "db76f6e0-9df0-4e5c-d310-a2e4196c9b4a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#@ Loading the Saved Model:\n",
        "model = Siamese()\n",
        "model.init_from_file(\"/content/model/model.pkl.gz\")\n",
        "\n",
        "#@ Model Evaluation: \n",
        "def classify(test_Q1, test_Q2, y, threshold, model, vocab, data_generator=data_generator, batch_size=64):\n",
        "  \"\"\" Function to test the Accuracy of the Model. \"\"\"\n",
        "  accuracy = 0                                                                               # Initializing the Accuracy.\n",
        "  for i in range(0, len(test_Q1), batch_size):\n",
        "    q1, q2 = next(data_generator(test_Q1[i:i+batch_size], test_Q2[i:i+batch_size],\n",
        "                                 batch_size, vocab[\"<PAD>\"], shuffle=False))\n",
        "    y_test = y[i:i+batch_size]                                                               # Using batch size of actual output target.\n",
        "    v1, v2 = model((q1, q2))                                                                 # Using the Model.\n",
        "    for j in range(batch_size):\n",
        "      d = np.dot(v1[j], v2[j].T)                                                             # Calculating the Cosine Similarity.\n",
        "      res = d > threshold\n",
        "      accuracy += (y_test[j] == res)\n",
        "  accuracy = accuracy / len(test_Q1)\n",
        "  return accuracy\n",
        "\n",
        "#@ Computing the Accuracy of the Model:\n",
        "accuracy = classify(Q1_test, Q2_test, y_test, 0.7, model, vocab, batch_size=512)             # Calculating the Accuracy.\n",
        "print(\"Accuracy of the Model:\", accuracy) "
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy of the Model: 0.75107421875\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZXjbgA7F-A0W"
      },
      "source": [
        "Now, I will test the Model using my own Questions. I will build a reverse Vocabulary that allows the map encoded Questions back to words. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mZ1ZiX4B99lf"
      },
      "source": [
        "#@ Model Evaluation with own Questions:\n",
        "def predict(question1, question2, threshold, model, vocab, data_generator=data_generator, verbose=False):\n",
        "  \"\"\" Function for predicting if two Questions are Duplicates. \"\"\"\n",
        "  q1 = nltk.word_tokenize(question1)                                # Tokenization.\n",
        "  q2 = nltk.word_tokenize(question2)                                # Tokenization.\n",
        "  Q1, Q2 = [], []\n",
        "  for word in q1:\n",
        "    Q1 += [vocab[word]]                                             # Encoding.\n",
        "  for word in q2:\n",
        "    Q2 += [vocab[word]]                                             # Encoding.\n",
        "  Q1, Q2 = next(data_generator([Q1], [Q2], 1, vocab[\"<PAD>\"]))\n",
        "  v1, v2 = model((Q1, Q2))                                          # Using Model.\n",
        "  d = fastnp.dot(v1[0], v2[0].T)\n",
        "  res = d > threshold\n",
        "  if (verbose):\n",
        "    print(\"Q1 = \", Q1, \"\\nQ2 = \", Q2)\n",
        "    print(\"d = \", d)\n",
        "    print(\"result = \", res)\n",
        "  return res "
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FQbSmPDY-E22",
        "outputId": "eda3ac88-57d3-4b53-e2ae-e9b8134381d4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        }
      },
      "source": [
        "#@ Examples of Questions:\n",
        "question1 = \"How are you?\"\n",
        "question2 = \"How are you doing?\"\n",
        "#@ Predicting the Duplicated Questions:\n",
        "example1 = predict(question1, question2, 0.6, model, vocab, verbose=True)\n",
        "print(\"Example1:\", example1, \"\\n\")\n",
        "\n",
        "#@ Example of Questions:\n",
        "question1 = \"Do you enjoy eating the dessert?\"\n",
        "question2 = \"Do you like hiking in the desert?\"\n",
        "#@ Predicting the Duplicated Questions:\n",
        "example2 = predict(question1, question2, 0.6, model, vocab, verbose=True)\n",
        "print(\"Example2:\", example2) "
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Q1 =  [[32 87 53 21  1  1  1  1]] \n",
            "Q2 =  [[  32   87   53 1438   21    1    1    1]]\n",
            "d =  0.8786118\n",
            "result =  True\n",
            "Example1: True \n",
            "\n",
            "Q1 =  [[  443    53  3158  1169    78 29071    21     1]] \n",
            "Q2 =  [[  443    53    60 15323    28    78  7438    21]]\n",
            "d =  0.5000767\n",
            "result =  False\n",
            "Example2: False\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}